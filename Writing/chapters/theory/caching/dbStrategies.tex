\section[Database Caching Strategies]{Database Caching Strategies}

There are multiple database caching strategies that could be applied when designing cache systems.
Depending on the type of request (read, write) of the user, and which part of the application is responsible for fetching data from the database or managing the cache,
there are five main database caching strategies.\\


% \subsection{Cache Aside}
\noindent \textbf{Cache Aside}

Cache Aside is also called Lazy Loading. The application is in control of managing the cache. Let’s look at the diagram below.

% \begin{center}
% (put cache aside diagram)
% \input{myFigures/cacheAside.tex}
% \end{center}

\input{myFigures/FiguresCommand.tex}

\cacheAsideFigure

As the diagram shown above, the client first checks the cache to see if the read request data is in the cache.
If the cache exists, this is called a cache hit, and the client uses it right away.
If the cache does not exist, this is called a cache miss. When a cache miss occurs, the client then sends a request to the database server to fetch the data.
After that, the client transfers the data to the cache, so that there could be a cache hit the next time the data is needed.
This will increase the performance of the application when the same data is called multiple times.
When there is a write request, the application will first communicate with the database and then update the cache.

This strategy is useful when the application requires a lot of read requests from the database,
since the application could just check the cache instead of querying the database.
One disadvantage of this approach is that there could be an inconsistency between the cache and the database.
Data directly written in the database might not be consistent with the data in the cache,
since writing data to the database and writing data to the cache does not happen at the same time.\\

% \subsection{Read-Through}
\noindent \textbf{Read-Through}

In the read-through strategy, the cache level manages fetching data from the database. Here is a diagram of how the read-through strategy works:

% \begin{center}
% (put read-through diagram)
% \end{center}

\readThroughFigure

Whenever there is a read request from the application, the application first checks the cache. If there is a cache hit, it simply returns the data back to the application. If there is a cache miss, the cache level fetches the data from the database. Since the cache manages fetching data, it simplifies the application logic when retrieving data compared to when the application handles fetching data.

This strategy only involves read requests from the application, which means that other strategies could be used for write requests.
The write-through strategy is generally great to be used with the read-through strategy.\\

% \subsection{Write-Through}
\noindent \textbf{Write-Through}

The write-through strategy is very similar to the read-through strategy. The diagram is almost identical.

% \begin{center}
% (put diagram)
% \end{center}
\writeThroughFigure

Nothing happens when there is a read request from the application, but when there is a write request, the data will first be written in the cache and then into the database. This ensures data consistency when paired with the read-through strategy. One downside of this strategy is that since it writes to both the cache and the database layer, the latency of the request increases.

The read-through and the write-through strategies are not meant to be used for all access to data in the application layer.
Using these strategies for all database queries could result in a decrease in performance, since the caching layer is not intended to keep every single data.\\

% \subsection{Write-Back}
\noindent \textbf{Write-Back}

The write-back strategy is similar to the write-through strategy, but the writes to the database happen with a delay. The cache is still in charge of writing data to the database. Let’s look at the diagram:

% \begin{center}
% (write-back diagram)
% \end{center}
\writeBackFigure

Since the cache only writes to the database after a certain amount of time,
this strategy is beneficial when there are multiple write requests to the same data at the application level.
This will decrease the write queries from the cache level to the database, which could increase the performance.
However, similar to the read-through and write-through strategies, since the data is written to the cache first,
writing data to the database could potentially fail if the caching fails.\\

% \subsubsection{Write Around}
\noindent \textbf{Write Around}

This strategy is similar to the cache-aside strategy, but it has more specific instructions for write requests.

% \begin{center}
% (write-around diagram)
% \end{center}
\writeAroundFigure

The difference between the write-around strategy and the cache-aside strategy is that when there is a write request, only the data in the database is updated. The data in the cache is only updated when there is a cache miss from a read request. This way, the cache is not overflooded with unnecessary data. The disadvantage of this strategy is that recently written data will always result in a cache miss. This is why the write-around strategy is suitable when the application has a lot of read requests and data is rarely updated.

\section[Redis]{Redis}
% https://learning.oreilly.com/library/view/redis-in-action/9781617290855/kindle_split_011.html
Until now, we have learned the types of database caching strategies. But how are these strategies actually implemented?
Redis is a widely used open-source data structure store that makes these caching patterns practical at scale.

Redis works entirely in memory, which gives it extremely fast read and write speeds.
Most databases store data on disk, so every query involves slower I/O operations.
Redis avoids this by keeping data in RAM, allowing applications to access cached values in microseconds.
This speed makes it perfect for handling database caching strategies \cite{Carlson_2013}.

Redis also supports a wide range of data structures—strings, hashes, lists, sets, sorted sets, and more.
These structures map naturally to different caching scenarios. For example, strings work well for caching simple query results,
hashes can store user profile objects, and sorted sets can maintain real-time rankings or leaderboards.
Because Redis supports many data structures, it is not limited to simple key–value caching.
It can also function as a fast and lightweight data engine that handles situations that require complex caching.

Reliability is another reason Redis is chosen for database caching. Redis provides persistence options (RDB snapshots and AOF logs)
so cached data or application-critical information can survive server restarts. It also supports replication and clustering,
which distribute data across multiple nodes to achieve both high availability and horizontal scalability.
These features ensure that the caching layer does not become a single point of failure.

In summary, Redis transforms abstract caching strategies into practical and high-performance solutions.
Its low latency, adaptable data model, and robust operational features make it a dependable component in web applications.
These are the reasons why Redis will be used to implement database caching strategies in this project.