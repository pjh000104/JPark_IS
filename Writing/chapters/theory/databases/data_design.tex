\section{Database Design and Evolution}
The design of databases and querying the database are the ones that have the most impact on the performance of the database.
This section investigates strategies to design the database to increase performance and efficiency,
consider scalability, evolution of databases, and ways to optimize the database and queries.

\subsection[Normalization]{Normalization}
Compared to a non-relational database, relational databases benefit from integrity with the process of normalization.
The definition of normalization is a way of organizing data in a database.
During this process, redundant data will be reduced to improve data quality and optimize database performance.
There are several levels of normalization, and as the normalization form gets higher,
it generally indicates a more refined relational design. However, most databases tend to be needed until the third normal form,
which avoids most of the problems common to bad relational designs \cite{Harrington_2016}.\\

\textbf{The First Normal Form (1NF)}

The two criteria that 1NF meet are the following:

\begin{enumerate}
    \item The data are stored in a two-dimensional table.
    \item There are no repeating groups. 
\end{enumerate}
% (rddI chapt 7) \cite{Harrington_2016} 

Here, the repeating groups mean that if an attribute has more than one value in each column of a row \cite{Harrington_2016}. 
% (relational database design and implementation 4th) 
For instance, if there is a column that requires more than one value, such as items (instead of item), this will be a repeating group.

\input{tables/1nfbefore.tex}

This table shows the example of a table before the first normalized form. We can see that students have multiple classes in the same column separated by a comma.
Having three or more dimensions or having repeated groups for a single table results in more complexity and difficulties when querying the database. 
This is why we need the 1NF normalization. The 1NF is the simplest normalization that could be done in a relational database, and it is pretty clear. 

\input{tables/1nfafter.tex}

As we can see on the table above, there is only one course in the course column by creating a new record for each item.
However, the first normalization form is not enough to benefit from using relational databases.\\

\textbf{Problems with 1NF}

Data could become redundant even though repeated rows are deleted. 
For example, if there is a table called students with columns(id, name, birthday, course id, course name), every time the same student adds a class, 
their name will be repeated in each record. Furthermore, there are anomalies in update, insertion, and deletion. 
Making an update to one of the records (student name) 
would require updating other records that are associated. Missing any of the records with the student name will result in data inconsistency. 
Inserting a new entity could be difficult unless there is related data. For instance, adding a new course to the table is not possible until
there is a student taking the course. This is because the primary key will be the student’s id, and there could not be a row without a primary key. 
Deletion could also be a problem when the row deleted contained the last data for a certain column. For example, 
if one student is taking a class called CS200, and that record is deleted, the table would not have the information that the class CS200 exists. 
To solve these problems, we must use higher levels of the normalization form.\\


\textbf{The Second Normal Form(2NF)}

The two criteria that 2NF meet are the following:
\begin{enumerate}
    \item The relation is in first normal form
    \item All non-key attributes are functionally dependent on the entire primary key. 
\end{enumerate}

Functional dependency is the key term in the 2NF. A functional dependency is a one-way relationship between two attributes, 
such that at any given time, for each unique value of attribute A, only one value of attribute B is associated with it throughout the relation \cite{Harrington_2016}.
% (RDDI chapt7). 
In other words, this means that all other columns except the columns of the primary key or keys, are dependent on the primary key or the candidate key.

By using the functional dependencies, we could create the second normal form relations. After analyzing the functional dependencies,
primary keys would be decided. It is common to decide on attributes that have dependencies
for the primary key of the table, and the all the other attributes will be the non-key attributes.
For example, going back to the student table example, the Student name and birthday would be dependent on the student id, so the student id will be the primary key.
The courses the student takes do not depend on the student id, but rather on the course id.
We could create another table using the course id as the primary key and the course name as the functional dependencies.
However, the courses the student takes will be dependent on the courses, so there will be a foreign key to construct that relationship.
So far, we can identify the relationship of the tables as the following:

\begin{itemize}
    \item \textbf{Student}(\textit{student\_id} (PK), \textit{course\_id} (FK), \textit{name}, \textit{birthday}, \textit{room\_num}) 
    \item \textbf{Courses}(\textit{course\_id} (PK), \textit{name})
\end{itemize}

\noindent
*We are assuming that each student can only take one course.*

\noindent
The relationship can be represented as:
\[
\text{Student}(\text{course\_id}) \rightarrow \text{Courses}(\text{course\_id})
\]

\input{tables/2nfbefore.tex}
\input{tables/2nfafter.tex}

By doing this, some problems from the first normal form are solved.
The functional dependency solves the insertion, deletion, and update anomalies.
We can now insert a new course into the course table without needing to insert a student at the same time,
delete a student’s course without losing the record of the course itself, and update course information in one place instead of multiple rows,
which preserves data integrity. This will provide a more stable and consistent database design compared to 1NF. \\

\textbf{Problems with 2NF}

Although some of the problems with 1NF were solved, there are still anomalies to be resolved. Insertion, deletion, and update anomalies still exist. 
For example, let us assume we have the following dependencies and the table:

\begin{itemize}
    \item \textbf{Student}(\textit{student\_id} (PK), \textit{course\_id} (FK), \textit{name}, \textit{birthday}, \textit{room\_num}) 
    \[
    \text{Student}(\text{course\_id}) \rightarrow \text{Course}(\text{course\_id}, \text{name}, \text{room\_num} (FK))
    \]
    
    \item \textbf{Course}(\textit{course\_id} (PK), \textit{name}, \textit{room\_num} (FK)) 
    \[
    \text{Course}(\text{room\_num}) \rightarrow \text{Room Number}(\text{room\_num} (PK), \text{name})
    \]
    
    \item \textbf{Room Number}(\textit{room\_num} (PK), \textit{name})
\end{itemize}

\input{tables/3nfbefore.tex}

Even though the tables are in 2NF, insertion, deletion, and update anomalies can still occur due to these transitive dependencies.

The functional dependency Course → Room Number introduces anomalies that still remain in 2NF.
For example, we cannot record a student’s enrollment in a course if the course’s room number has not been decided, which creates an insertion anomaly.
Also, if we delete the last student enrolled in a course, we also lose the record of the room number for that course, which is a deletion anomaly.
Finally, if a course’s room number changes, we must update it in every student’s record for that course, creating an update anomaly.
These problems occur because the room number depends on the course (a non-key attribute) rather than directly on the student ID,
and they are resolved by moving the design into 3NF. \\

\textbf{The Third Normalization Form(3NF)}

The two criteria that 3NF meet are the following:

\begin{enumerate}
    \item The relationship is in second normal form.
    \item There are no transitive dependencies.
\end{enumerate}

\textbf{Transitive dependencies}

Transitive dependencies exist when the following functional dependency pattern occurs:  
\[
A \rightarrow B \text{ and } B \rightarrow C \text{, therefore } A \rightarrow C
\] 
% \cite{Harrington_2016}
% (Chapter 7).  

This is the same type of relationship where we had problems with 2NF. For example:

\begin{itemize}
    \item \textbf{Student}(\textit{student\_id} (PK), \textit{course\_id} (FK), \textit{name}, \textit{birthday}, \textit{room\_num}) 
    \[
    \text{Student}(\text{course\_id}) \rightarrow \text{Course}(\text{course\_id}, \text{name}, \text{room\_num} (FK))
    \]
    
    \item \textbf{Course}(\textit{course\_id} (PK), \textit{name}, \textit{room\_num} (FK)) 
    \[
    \text{Course}(\text{room\_num}) \rightarrow \text{Room Number}(\text{room\_num} (PK), \text{name})
    \]
\end{itemize}

Going back to this example, we can see that a non-key attribute in the \textbf{Student} table (\textit{room\_num}) depends on another non-key attribute (\textit{course\_id}).  

To remove transitive dependencies, we should break the relations into separate tables. In this case, we can:

\begin{itemize}
    \item Remove the \textit{room\_num} column from the \textbf{Student} table to remove the relationship between non-key attributes.
    \item Alternatively, make the second determinant in a table a candidate key so that no non-key attribute depends on another non-key attribute within the same table.
\end{itemize}

Applying either method will resolve the anomalies we encountered in the second normalization form (2NF).

\input{tables/3nfafter.tex}




\subsection[Database Evolution]{Database Evolution}

Database design also influences how a database evolves over time. Performance, latency, and fault tolerance are affected by the database designs as the amount of stored data increases. 
There are multiple techniques to design a database suitable for scaling. 
Vertical scaling refers to when the physical parts of the computer such as CPU or RAM are upgraded, so the database can handle more data and queries. 
Horizontal scaling is when multiple machines with independent physical parts (CPU and RAM), store the data on an array of disks that are shared among multiple other machines,
which is also called nodes. 
One advantage of using vertical scaling is that the structure is very simple. Only one database needs to be taken care of, 
which could be easier compared to handling multiple databases. However, there are more disadvantages. 
Cost increase faster than linearly as higher performance is required. Fault tolerance is problematic as there is only one database running, 
hardware or software failures on the database can cause service interruptions.
For horizontal scaling, there are mainly two strategies that we will discuss: 
replication and partitioning. These two strategies have its own sub strategies which will be discussed in the next subsection. \\

\subsubsection[Replication]{Replication}

Replication is when there are multiple copies of the database in different nodes, which could be in different virtual, or physical machines.
This leads to redundancy in data, but it also allows for fault tolerance if one of the database hosts is unavailable \cite{Kleppmann_2017}.
% (Designing Data Intensive ch5)
Another reason to use replication is to increase availability by having databases in different regions to decrease latency.
Having multiple database to read queries also help increase the performance of the application as the size of the database grows.

There are multiple ways to implement this strategy in different situations. The two topics discussed will be Leader and Followers and Multi Leader Replication. \\

% \subsubsection[Leaders and Followers]{Leaders and Followers}
\noindent \textbf{Leaders and Followers}
\input{myFigures/FiguresCommand.tex}

Going deeper into how the replicas are established, it is important to ensure that each of the replicas have the same data.
For example when a user updates a table, this should include all the other replicas to update the same table.
Here is an example how the leader follower replication setup could look like:

\leaderFollowerFigure

The leader and follower structure has one lead node (leader) that allows writes to the database,
and the rest of the nodes are assigned as followers. If there is a change data in the leader, the followers are updated accordingly.
Reading the database could be done by any of the nodes including the leader node,
but writing the database could only be done by through the leader node. \\


% (add an example figure) \\

% \subparagraph{Multi-Leader Replication}
\noindent \textbf{Multi-Leader Replication}

One downside of a Leader and Follower replication was that write queries were only allowed in the leader replica.
If the application could not connect to the leader for any reason such as network issues, write queries could not be performed.
Multi-Leader replication allows multiple leaders, which means that write queries could be done in multiple replicas. 
Although it is rare for a multi-leader replication structure to outweigh the benefits against the complexity added, there are a few use cases for this structure.
\begin{itemize}
    \item Multi datacenter operation
    \item Clients with offline operation
    \item Collaborative editing
\end{itemize}

In situations where there are multiple datacenter operation, multi-leader replication allows each datacenter to process write requests locally,
which will reduce latency and improve availability during datacenter or network failures. Also, for applications that are necessary to support offline operation, 
each device can function as an independent leader, enabling uninterrupted read and write access when disconnected. If this were to be a single-leader replication, 
clients would not be able to perform write operations offline, limiting usability in environments with low network connectivity. 
Finally, in collaborative editing systems, multi-leader replication enables multiple users to make concurrent updates with low latency. 
If using a single-leader replication, all edits would need to pass through the leader, and clients would need to wait for others to finish their changes.
This will increase latency and reduce the responsiveness of real-time collaboration \cite{Kleppmann_2017}.

However, the biggest issue in the multi-leader replication structure is handling multiple write query conflict.
For instance, if a same record was modified by multiple users on different leader replicas, 
there will be different data among replicas resulting in data inconsistency.
This was not an issue for a single leader and followers structure since only one leader accepted write queries, and they were processed one by one. However, if multiple leaders receive queries at the same time, the conflict should be resolved. There are multiple ways to handle write conflicts.

The simplest strategy is to just avoid the conflicts. If write queries could be avoided in the application layer,
where the application ensures that all write queries for a particular record go through the same leader.
This would work until a particular records needs to change its designated leader. In this case, simply avoiding conflicts in the application layer would not work

The next strategy is to converge toward a consistent state. When there is a situation where avoiding conflicts do not work, 
the conflicts should be handled. There are various ways to converge the replicas into a consistent state.
One way could be to simply give each write query a unique ID, and if there is a conflict, 
the higher unique ID overwrites the conflict. Another way is to give each replica a unique ID, 
and the higher unique ID overwrites the conflict.
Or, there could be another explicit data structure that preserves the information and write application code to resolve 
the conflict in a certain logic. There are trade-offs such as which part of the data is going to be saved over the other, 
so it is essential to choose the write way to handle write conflicts in a multi-leader replica structure. \\

% Replication is a way to not only have fault tolerance but also improve the performance of the database. This will be useful as the size of the database and the application grows. It is essential to choose the right strategy for replication depending on the situation to support the evolution of databases and maximize performance.

% \subparagraph{Synchronous vs Asynchronous Replication}
\noindent \textbf{Communication between replicas: Synchronous vs Asynchronous Replication}

Communication that happens between the leaders and the followers is also another important factor to decide whether it is a
leader and follower setup or multi-leader setup. The two types of communication are synchronous and asynchronous.
After the leader receives a write query, synchronous replication waits until all the followers are updated and synced.
The leader will not take any queries before this process is finished.
However, asynchronous replication keeps taking queries regardless of the status of the followers.
Although handling requests to copy status of the leader on the database replicas are fast, which usually happens less than a second \cite{Kleppmann_2017},
% (Designing Data-Intensive Applications ch 5),
there is no guarantee of how long it would take depending on the physical status of the replica and the network.
This is an advantage of synchronous replication, since it is certain that all database replications are up-to-date with the leader database,
but it could be a disadvantage in a perspective where the database queries will be slow.
If all of the followers are synchronous, the system would be too slow. Most of the times,
one of the followers will be set to being synchronous so that there will be at least two replications(one leader, one follower)
up-to-date with all the write queries, and the rest of the followers being asynchronous. This configuration is sometimes also called semi-synchronous.(Designing Data-Intensive App ch5) 

Asynchronous replications on the other hand are very fast since the leader does not have to wait until the followers have synced data.
However, it does have a disadvantage that all of the followers might not be up-to-date right away, which is also known as the replication lag.
This might sound like a huge disadvantage, but asynchronous replication is often times used if there are many followers or if they are geographically distributed.
Eventually, all the followers will have a synced replication of the leader, since inconsistency is just a temporary state.
This effect is called as eventual consistency \cite{Kleppmann_2017}.
% (Designing Data-Intensive App ch5).
If the application that is being created is okay with possibility of inconsistency due to replication lag,
asynchronous replication is perfect. However, if consistency is crucial to the application, other solutions such as synchronous or semi-synchronous
replication will be the go-to when using leaders and followers method. 


\subsubsection{Partitioning}

Partitioning is when a data is divided into multiple smaller pieces of data such as a record, a row, or a document.
Replication was used for scalability and performance, but as the size of the database increases, 
only replication itself become inefficient since all the data has to be copied to the replicas.
In order to scale database applications, partitioning is commonly used.
As the data is distributed among multiple partitions, multiple queries could be run at the same time. \\

% \subparagraph{Partitioning by Key-Value Data}
\noindent \textbf{Partitioning by Key-Value Data}

Having a key-value data is one way to create partitions. The reason for partitioning is to scale the database by distributing the query load on one node.
One way to achieve this is to have a key range for each partition. For example, when we think of a dictionary,
we know that words that start with the letter “a” is in the beginning, the letter “m” is somewhere in the middle, and the letter “z” is at the end.
By dividing the range of partitions by the key-value data, it will be easier to locate which partition contains the data and distribute the query.
In this case, the partitions might not be evenly divided since each range of key-value pairs could contain a different amount of data.
It is important which key-value pair to use. If a key is assigned as a column that is not evenly distributed, this leads to a hot spot 
(a partition with disproportionately high load) \cite{Kleppmann_2017}.
% (Designing Data-Intensive App  ch6). 

A method that has a lower chance of being affected by hot spots is partitioning by hash of key.
A good hash function makes the key-value pairs evenly distributed among the partitions.
However, this loses the efficiency that the key range had, since keys that are close in value may be distributed across different partitions.
This leads to range queries to access multiple partitions and merge their results, which will increase query latency.  

When choosing the strategy, hashing keys and having key ranges to partition data both have advantages and trade-offs,
so it is important to choose the one suitable in the situation.
Key range could be used if there is a key range that is evenly distributed, 
and key hash when there are still a lot of hotspots by using the key range. \\

% \subparagraph{Partitioning and Secondary Indexes}
\noindent \textbf{Partitioning and Secondary Indexes}

Partitioning by key-value pairs work nicely with the partitioning strategies discussed above. However, things get more complicated with secondary indexes. Secondary indexes will be discussed more in the upcoming sections. A brief introduction for secondary indexes is that they are created to improve the query performance that are based on multiple keys. For example, when searching for a job in a job board, selecting criteria such as industry or experience level could be a situation to use secondary indexes.
Secondary indexes make partitioning more complicated since they don’t identify a single record, but rather search for records based on the conditions. Partitioning by key-value pairs wouldn’t work in this situation.

Scatter/gather is a technique where each partition has its own secondary indexes. In this case, writing data would be efficient since only the partition containing that specific part of the index is where the data is written. However, reading queries are less efficient.  If queries require searching on different partitions, each partition is searched separately which is inefficient. 

It is also possible to partition secondary indexes by the term.
Instead of having distributed secondary indexes across the partitions, 
this approach has a global index that covers all the partitions.
Having a range of secondary indexes separated in multiple partitions make read queries more efficient 
from the scatter/gather technique. 

Based on the situation, it is important to choose the correct technique. 
If write queries are used more often, it is better to use the scatter/gather technique, whereas if read queries are dominantly used,
partitioning indexes by term is the structure to choose. \\

% \paragraph{Request Routing}
\noindent \textbf{Request Routing}

After partitioning the dataset into multiple nodes, it is important to make sure the request connects to the right partition. On a high level there are mainly three ways to achieve this. 
\begin{enumerate}
    \item Allow clients to contact any node (e.g., via a round-robin load balancer, which distributes incoming requests evenly across nodes without considering partition ownership.). If that node coincidentally owns the partition to which the request applies, it can handle the request directly; otherwise, it forwards the request to the appropriate node, receives the reply, and passes the reply along to the client.
    \item Send all requests from clients to a routing tier first, which determines the node that should handle each request and forwards it accordingly. This routing tier does not itself handle any requests; it only acts as a partition-aware load balancer.
    \item Require that clients be aware of the partitioning and the assignment of partitions to nodes. In this case, a client can connect directly to the appropriate node, without any intermediary \cite{Kleppmann_2017}. 
    % (Designing Data-Intensive App Ch 6)
\end{enumerate}

The key problem is that how does the component, (which could be the routing tier, the client, or the partition in the node) know about changes in the other partitions in other nodes? If the deciding component is not up-to-date with the location where data that the client is requesting, this could lead into data inconsistency. 

To solve this problem, many distributed data systems use a separate coordination service such as the ZooKeeper, which registers each node
and maintains the authoritative mapping of partitions to nodes, to keep track of the data \cite{Kleppmann_2017}.
Whenever a partition changes or a node is added or removed the ZooKeeper is updated to keep data consistency. 

