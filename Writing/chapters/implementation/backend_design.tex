\section[Backend Design]{Backend Design}\label{sec:newsec}
Multiple options are available when selecting a web framework for backend development. Java Spring Boot is chosen for this project for two reasons.
First, this project provided an opportunity to explore a backend framework beyond Python Flask, which was the only framework I have previously used.
When I was choosing my framework at the beginning of the project, I planned to go back to South Korea after graduation.
I wanted to choose a framework commonly used in the software industry in South Korea, which was Java Spring Boot. Other backend 
web frameworks, such as Django and NestJS, are used in South Korea, but Java is the most dominant, making it practical and relevant to the industry \cite{Prime_Career}.

The second reason for selecting Java Spring Boot emerged during the development process. 
It became clear that Spring Boot integrates effectively with the chosen software architecture,
particularly in terms of modular design. Java's structured organization around pacakges, classes, and interfaces aligns naturallly with modular boundaries.
Each domain (cuisine, region, reviews) can be encapsulated within its own package hierarchy, promoting high cohesion within modules and reduced coupling between them.
This architectural compatibility is discussed in greater detail in a later subsection. 

Beyond the backend framework, additional technologies were selected to support development efficiency and scalability.
Supabase was chosen as the database hosting solution to leverage a managed cloud service, allowing the project to focus on application
logic rather than data and infrastructure management. Redis was selected as the caching solution due to its widespread industry adoption,
proven performance, and suitability for implementing efficient caching strategies within the application.

\subsubsection{Software Architecture}

\noindent \textbf{Monolithic vs Distributed}

Multiple design approaches were considered when deciding whether the local cuisine application should adopt a monolithic architecture or a distributed system architecture.
Until now, all the applications that I have created were monolithic applications. As a student, the scope of solo projects were small enough that 
monolithic architectures were appropriate, and all software developed or contributed to during internships also employed a monolithic architecture.

For this reason, I wanted to create an application that has a distributed system architecture in the beginning. I wanted to try something that I have not tried,
and also gain experience using this architectural pattern. Developing an application using both monolithic and distributed architectural styles provides valuable
perspective when selecting an appropriate software architecture, as it enables a deeper understanding of the trade-offs involved in architectural decision-making. 

However, as I continued exploring architectural style for the local cuisine application, the monolithic architecture seemed to be a more compelling option.
I plan to keep maintaining and expanding on this project even after graduation and the final submission of the independent study.
Through the study of software architecture theory, it became clear that adopting a distributed system architecture at an early stage could introduce unnecessary complexity,
which may not be justified for a system of this scale. 
The benefits that I would gain by setting up the distributed system architecture outweigh the drawbacks.

First, Modular monolithic architecture is appropriate for the local cuisine application compared to the distributed system architecture in terms of scaling.
Scalability is one of the primary advantages of a distributed system architecture. It is possible that the local cuisine application could grow significantly and attract thousands of daily users,
and a distributed architecture could provide clear benefits in handling increased traffic and workload. However, in the current stage of development where the aim of 
average users is less than thousands of users, the modular monolithic architecture is easier to maintain the application.  

Furthermore, as the local cuisine application is still relatively small, it would not benefit from the loose coupling that the distributed system architecture has.
It is possible to divide each part of the application into different services, such as having an individual service for cuisines, regions, and reviews. 
However, introducing this level of separation would be unnecessary at the current stage unless the applications scales.
The distributed architecture approach becomes more valuable only when the application scales significantly and requires independently evolving services.
For the present scope of the local cuisine application, the advantages of loose coupling provided by a distributed architecture do not outweigh the added complexity
such as inter-service communication or network latency.

Another big reason the monolithic architecture was chosen was the cost. Although a distributed system architecture offers several advantages,
the associated infrastructure costs are critical. Deploying and maintaining multiple servers to support distributed services can rapidly increase the cost,
that exceeds the scope of a personal project. On the other hand, the monolithic architecture only needs one server, which is cost-effective and easier
to maintain over the long term.

Finally, the modular monolithic architecture was chosen since it preserves the possibility of futer evolution into a distributed system architecture.
In case the local cuisine application needs to scale, the existing modular structure provides two options: maintain the current monolithic deployjment, 
or incrementally refactoring modules into independent services.
The clear separation of concerns within the modular monolith allows potential transition, as the well-defined module boundaries can serve as a foundation for service extraction.
This flexibility allows architectural decisions to be revisited based on observed system growth, performance requirements, and usage patterns, rather than being determined from the beginning.

Fault tolerance was the main trade-off considered in the architectural decision-making process.
While a distributed system architecture provides stronger fault tolerance,
the overall requirements and constraints of the project led to the selection of a monolithic architecture for the local cuisine application.

\noindent \textbf{Layered or Modular Monolithic}

There are two monolithic architectures introduced in the theory section: the Layered architecture and the Modular Monolithic architecture.
For the local cuisine application, I decided to use the modular monolithic approach.
The main reason behind this decision is the clear separation of concerns it provides between different parts of the application.

The application is composed of three main components: regions, which allow users to explore the geographic areas supported by the local cuisine application;
cuisines, which provide detailed information about the culinary offerings within a selected region;
and reviews, which enable users to interact with one another by submitting and viewing feedback on specific cuisines.

The modular architecture is well-suited to this application, since the local cuisine application
has a clear separation of functional domains: region, cuisine, and reviews. If this were implemented using a layered architecture,
the application would be organized strictly by technical layers (such as controller, service, and repository)
rather than being structured around the distinct functional domains of the system. 
This structure not only mirrors the real-world organization of the application but also improves maintainability by isolating changes to individual modules
and reducing the risk of unintended side effects across the system.

Furthermore, the modular monolithic architecture could also retain the organizational benefits of a layered architecture.
For example, Within each module, technical layers—such as presentation, workflow, persistence, and database—can be implemented independently,
ensuring clear separation of concerns and consistent layering practices across the system.
Each module could contain these layers within its own module, which allows modular monolithic inherit the advantages 
of the layered architecture. Here is the structure of the modular monolithic architecture for the local cuisine application.

% Include directory tree after pushing.

The figure above shows three modules in the local cuisine application: the region, cuisine, and reviews.
Within each module, the API, service, and domain packages are included. The api package is a part of the presentation layer, where 
information is organized and sent to the front end. The service package is the workflow layer where all the business logic is included.
Finally, the domain package is the persistent layer where entities are mapped persistently to the relational database.

This architecture of the modular monolithic structure allows separation of modules and layers, which makes it easier to maintain the code.
The separation is also beneficial when the application would be developed collaboratively in the future as well.

However, it is possible that the decision to implement the modular monolithic architecture for this project 
could be viewed as excessive. There are only three modules for the local cuisine application, and it is possible to 
just have all of these modules under the layers of the layered monolithic architecture. 
Although it is currently a small-sized application, the decision to adopt a modular monolith was guided by the potential for future scalability,
allowing the system to evolve without requiring a major architectural overhaul. 
Moreover, I have already experienced the layered monolithic architecture throughout class
projects in the past, and I wanted to explore different structures to enhance decision-making skills.
Practical considerations, including personal learning goals and the desire to experiment with modular design, also informed this decision.

% Could include how it could be done with the micro service architecture?

\subsubsection{Database Systems}

\input{myFigures/FiguresCommand.tex}

% relational vs non relational


\noindent \textbf{ERD Diagram and Normalization}

\erdFigure

The database for the local cuisine application is designed to store three main pieces of information: the region, the cuisine, and the reviews.
Accordingly, there are three main tables: the regions table, the cuisines table, and the reviews table. 
As shown in the entity relationship diagram, the regions table has a one-to-many relationship with the cuisines table, 
since one region can contain multiple cuisines, while each cuisine belongs to a single region. 
The cuisines table also has a similar relationship with the reviews table. Each cuisine will have multiple reviews,
but each review is associated with one cuisine. 
These relationships are implemented using foreign keys. The region\_id in the cuisines table references the regions table,
and the cuisine\_id in the reviews table references the cuisines table.
The regions table includes information about the area, and has attributes id (primary key), country, name, and region. 
The cuisines table contains details about individual cuisines, including id, cuisineName,
and description, and references the associated region through region\_id.
The reviews table stores reviews for each cuisine and includes the attributes id, rating, comment, reviewer\_name, and created\_at.

To enforce the integrity of databases, normalization the data is crutial.
In the local cuisine database, each table satisfies the requirements of 3NF.
All non-key attributes in the regions table depend solely on the primary key region\_id.
Similarly, in the cuisines table, attributes such as cuisine name and description depend only on cuisine\_id,
while the region\_id is used as a foreign key to represent relationships rather than introducing transitive dependencies.
Finally, in the reviews table, attributes related to the reviews (such as rating and comment) depend only on the primary key review\_id.

As a result, the database structure avoids transitive dependencies, minimizes redundancy, and ensures data integrity.
This fulfills the requirements of the third normal form. The third normalization form for the database ensures that redundant storage of
region or cuisine data across multiple tables to reduce the risk of inconsistent data, and allows the database to scale without adding or 
restructuring existing tables. This database design also goes well with the modular design of the application
since each module (region, cuisine, and review) is clearly separated.

Currently, the region and cuisines table are filled with data from the Wikipedia API. The data was gathered through scraping using Claude AI.
There were other options when choosing the information for these tables such as external APIs or datasets. However, they did not include all the information
that the local cuisine application requires. For example, some APIs and datasets did not include the original region of the cuisine, or the description of the cuisine.
For the following reasons, the Wikepedia API was chosen since it included all the information needed for the local cuisine application, and was able to gather information
efficiently.
The reviews table includes sample reviews generated by artificial intelligence for testing purposes, but will be filled with human written reviews in the future.


\noindent \textbf{Optimization}

When testing the API endpoints of the local cuisine application, there weren't any significant performance issues when 
tested locally. However, considering that the database will grow and there will be multiple clients connecting to the API 
simultaneously in the future, a few database optimization techniques were implemented. 

The local cuisine application is designed that the cuisines and the reviews are searched the most and make database queries.
When the user searches for the cuisines, the cuisines table is accessed and the reviews of each cuisine are also loaded from 
the reviews table. Furthermore, the cuisines table currently has only about 30 rows, but will expand in the future.
The reviews table will also grow exponentially as the number of cuisines and the active users increase over time.
This is why the cuisines table and the reviews table need to be optimized.

Among the database optimization and the evolution techniques discussed in the theory section, indexing is used for the 
cuisines table. Indexing is applied to the cuisines table to improve search performance and reduce query execution time.
One of the core features of the system is retrieving all cuisines belonging to a specific region, and this requires frequent filtering using the region\_id attribute.

Without indexing, queries that filter cuisines by region would require a full table scan, resulting in increased response time as the number of records grows.
This is why an index was created on the region\_id column of the cuisines table. This allows the database to efficiently
locate relevant rows using an index scan instead of scanning the entire table.

The index was created using the following SQL statement:

\begin{verbatim}
CREATE INDEX idx_cuisines_region_id
ON cuisines (region_id);
\end{verbatim}

By indexing the foreign key attribute region\_id, the database can quickly retrieve all
cuisines associated with a given region and significantly improve performance for read-heavy operations.
This optimization is effective for the application, since cuisine data is queried frequently but updated infrequently.
As a result, the application has minimal write overhead while providing faster search and retrieval times. As the dataset scales, this indexing strategy
helps maintain consistent performance and improves the overall responsiveness of the application.

Another optimization done in the local cuisine application is avoiding the N+1 prohblem.
As mentioned in the query section, the N+1 query problem is a common performance issue in ORM-based applications, and this application uses the Hibernate ORM.
When fetching the cuisines, the cuisine repository uses the following gmethods.

\begin{verbatim}
public interface CuisineRepository extends JpaRepository<Cuisine, Long> {
    List<Cuisine> findByRegion_RegionName(String regionName);
    List<Cuisine> findByRegion_Country(String country);
}
\end{verbatim}

These methods generate SQL queries similar to 

\begin{verbatim}
-- Example for findByRegion_RegionName
SELECT c.id, c.cuisinename, c.description, c.region_id
FROM cuisines c
JOIN regions r ON c.region_id = r.id
WHERE r.region_name = ?;

-- Example for findByRegion_Country
SELECT c.id, c.cuisinename, c.description, c.region_id
FROM cuisines c
JOIN regions r ON c.region_id = r.id
WHERE r.country = ?;
\end{verbatim}

If the application were to access the region for each cuisine individually, such as calling \texttt{cuisine.getRegion().getRegionName()},
this could trigger one additional query per cuisine, resulting in the classical N+1 problem.
However, the local cuisine application avoids this entirely by performing all filtering and joining at the repository level. 

For example, the service method:

\begin{verbatim}
public List<String> getCuisinesByRegion(String regionName) {
    List<Cuisine> cuisines = cuisineRepository.findByRegion_RegionName(regionName);
    return cuisines.stream()
                   .map(Cuisine::getCuisineName)
                   .collect(Collectors.toList());
}
\end{verbatim}

only accesses the cuisineName field of each Cuisine entity. 
The findByRegion\_RegionName is a repository-level method that performs filtering and joining directly in the database, 
returning only the relevant Cuisine records. 
Since the related Region entity is never traversed in application code, no additional queries are executed per cuisine. 
This ensures that the number of executed queries remains constant regardless of the number of cuisines returned and effectively preventing the N+1 problem.

Furthermore, lazy loading is used for the region association in the Cuisine entity:

\begin{verbatim}
@ManyToOne(fetch = FetchType.LAZY)
@JoinColumn(name = "region_id", nullable = false) 
private Region region;
\end{verbatim}

When a Cuisine is fetched, only the region\_id foreign key is retrieved from the database, 
and the full Region entity is loaded only if it is explicitly accessed in the application. 

Of course, eager loading could be used instead of lazy loading to prevent any potential N+1 problems, but this approach would be inefficient because
all region data would be loaded for every cuisine, even if the region information is not needed for the operation. 

The combination of lazy loading and repository-level filtering allows the application to retrieve only the necessary data, 
which makes sure that the performance is efficient while still avoiding N+1 queries. 
The repository query returns all relevant Cuisine records in a single SQL statement, 
and the service layer only accesses the fields it needs. This will never traverse the Region entity unless explicitly required and avoid the N+1 problem.

Batch fetching was also not necessary in this design because the repository query already fetches all required data in a single call. 
The total size of the data (number of cuisines and regions) is small enough that performance is not a concern as of now. 

\noindent \textbf{Evolution}

To prepare for scaling the application, partitioning is applied to the reviews table.
The reviews table is partitioned through the partitioning by key-value pair technique discussed in the theory section.
The reviews table has the potential to have the most rows in the future, so scalability is crucial to this specific table.
Partitioning the reviews table will improve scalability and query performances by distributing data and query load across multiple nodes.
Most queries in the local cuisine application are centered around a single entity, such as retrieving cuisines by region or reviews by cuisine.
Partitioning by key-value allows requests to be routed directly to the partition that owns the relevant data.

Here is the query that is used to partition the reviews table. 

\begin{verbatim}
CREATE TABLE reviews_p0 PARTITION OF reviews
    FOR VALUES WITH (MODULUS 4, REMAINDER 0);

CREATE TABLE reviews_p1 PARTITION OF reviews
    FOR VALUES WITH (MODULUS 4, REMAINDER 1);

CREATE TABLE reviews_p2 PARTITION OF reviews
    FOR VALUES WITH (MODULUS 4, REMAINDER 2);

CREATE TABLE reviews_p3 PARTITION OF reviews
    FOR VALUES WITH (MODULUS 4, REMAINDER 3);
\end{verbatim}

The reviews table is partitioned using hash-based partitioning on the partition key. The \texttt{MODULUS 4} clause specifies that the table is divided into four partitions,
while the \texttt{REMAINDER} value determines which partition a row belongs to based on the hash value of the partition key modulo 4.
As a result, each review record is assigned to one of the four partitions as shown in the ERD diagram.

This will distribute the reviews data evenly across each partition, and also reduce the probability of hot spots. This way of partitioning will improve
scalability as data increases.
Hash-based partitioning also aligns with how the application accesses data in the database, since most queries retrieve reviews by a specific cuisine identifier.
By distributing reviews across partitions, query can be processed parallel, and improve query performance as well.

At the current stage of development, secondary indexes were not introdueced into the database design.
Although secondary indexes can significantly improve query performance when filtering or sorting data by multiple attributes,
they also add complexity to partitioned systems, particularly in terms of request routing and index maintenance.
In this application, reviews are mostly accessed through their associated cuisine, and there is no requirement to sort or
filter reviews by multiple criteria such as rating and reviewer name, or creation time and rating.

Since queries do not involve complex search conditions across multiple attributes, the benefits of secondary
indexes are limited in the current use case. Avoiding secondary indexes simplifies the database design and reduces overhead for write operations.
However, this does not mean that secondary indexes will not be implemented in the future. If the application scales and requires
filtering reviews by multiple criteria, secondary index could be incorporated to optimize read performance. This approach allows the system to remain efficient 
and easy to maintain while still being open for future optimization.

In this system, Supabase’s managed cloud database (PostgreSQL) is responsible for handling replication across database instances.
Supabase supports read replicas, which are additional databases kept in sync with the primary database using PostgreSQL’s
native replication mechanisms. Read replicas are asynchronously replicated from the primary database.
This means that changes on the primary are streamed to replicas without blocking writes, which helps avoid slowing down the primary
for scalability while still providing consistent read copies of the data. This approach aligns with the leader–follower pattern discussed earlier,
where the primary node handles writes and replicas handle read queries, improving performance and load distribution without introducing the complexity
of multi-leader conflict resolution. Supabase provides read replicas that remain read-only and reduce load on the primary to enable low-latency reads across regions
if multiple replicas are deployed \cite{Supabase_2026}.
% https://supabase.com/docs/guides/database/replication



\subsubsection{Caching}

Caching is implemented in the local cuisine application to enhance performance and 
reduce latency for frequently accessed data.  

Caching in the local cuisine application improves performance by reducing database access for frequently requested data.
By storing frequently accessed information in memory, such as cuisine names by region,
the application can serve responses faster, leading to a smoother user experience and reduced database load.
This is particularly effective for operations that are read-heavy, such as displaying lists of cuisines
when users browse popular regions.

Cuisine data and the reviews data implemented the caching strategy in the local cuisine application.
Among the caching strategy discussed in the theory section, the cache-aside pattern for the cuisine data.
As the cuisine data won't be written or updated by the users, all the caching strategies that involved writting was not applicable,
and the cuisine data is primarily read-heavy.
Unlike read-through caching, where the cache automatically loads data on a miss,
cache-aside allows the service to explicitly control when and how data is loaded and stored in the cache.
This approach provides flexibility for handling cache misses and controlling caching logic, which is ideal for our application
where writes are rare and controlled by the system.
Write-through or write-back, and write-around techniques were not applicable, as users do not modify cuisine data directly.  

For cache invalidation, a time-based expiration strategy was used. 
Cached entries are set to expire after a fixed duration (e.g., 1 hour) to ensure that the data remains reasonably 
fresh while still benefiting from caching. 
Event-based or validation-based invalidation strategies were not used because they require active
notifications or triggers when data changes. In this application, cuisine and region data are rarely updated,
so implementing event-based or validation-based invalidation would introduce unnecessary complexity without significant benefit.

The cache-aside strategy described above is implemented in the CuisineService class.
The service checks the cache before querying the database and stores the result
in the cache on a cache miss. This ensures that subsequent requests for the same region
can be served directly from memory.


\begin{verbatim}
public List<String> getCuisinesByRegion(String regionName) {

    String cacheKey = "region:" + regionName + ":cuisineNames";

    // 1. Check cache
    Object cached = cacheService.get(cacheKey);
    if (cached != null) {
        return (List<String>) cached;
    }

    // 2. Cache miss → query database
    List<Cuisine> cuisines =
            cuisineRepository.findByRegion_RegionName(regionName);

    List<String> cuisineNames = cuisines.stream()
            .map(Cuisine::getCuisineName)
            .collect(Collectors.toList());

    // 3. Store result in cache (1 hour TTL)
    cacheService.put(cacheKey, cuisineNames, 3600);

    return cuisineNames;
}
\end{verbatim}

The caching process follows the cache-aside pattern and is implemented in three steps.
First, a unique cache key is generated using the requested region name. This ensures that
each region’s cuisine list is cached independently.

Next, the application queries the cache using the generated key. If the data exists
(cache hit), the cached list of cuisine names is returned immediately,
bypassing the database entirely. This significantly reduces response time and database load.

If the data is not found in the cache (cache miss), the service queries the database
using the CuisineRepository. The retrieved cuisine entities are then transformed
into a list of cuisine names. Finally, the result is stored in the cache with time based invalidation of one hour before being returned to the client.

This explicit control over cache access and population reflects the cache-aside strategy,
allowing the application to balance performance improvements with data consistency while
keeping the caching logic simple and maintainable. 
The same caching approach is also applied to the getCuisinesByName method, which follows a similar structure
and purpose. This also ensures consistent performance improvements when accessing across different cuisines by the cuisine name.

For the review data, initially, a write-through caching strategy was considered. Because reviews are created and deleted by users, it seemed beneficial to keep
the cache fully synchronized with the database on every write operation.
Under this approach, each write operation would immediately update the cached review list, ensuring strong consistency between the cache and the database.
Here is the original code for the write-through caching strategy.

\begin{verbatim}
public ReviewResponse createReview(CreateReviewRequest request) {

    Cuisine cuisine = cuisineRepository.findById(request.getCuisineId())
            .orElseThrow(() -> new IllegalArgumentException("Cuisine not found"));

    Review review = new Review(
            cuisine,
            request.getRating(),
            request.getComment()
    );

    Review saved = reviewRepository.save(review);

    String cacheKey = "cuisine:" + cuisine.getId() + ":reviews";
    List<ReviewResponse> updatedReviews =
            reviewRepository.findByCuisineId(cuisine.getId())
                    .stream()
                    .map(r -> new ReviewResponse(
                            r.getId(),
                            r.getRating(),
                            r.getComment(),
                            r.getCreatedAt()
                    ))
                    .collect(Collectors.toList());

    cacheService.put(cacheKey, updatedReviews, 300);

    return new ReviewResponse(
            saved.getId(),
            saved.getRating(),
            saved.getComment(),
            saved.getCreatedAt()
    );
}
\end{verbatim}

We can see from that a created review is both written in the cache and the database.
This approach ensured that cached data remained fully up to date, but it introduced significant overhead.
Each write operation required an additional database query to rebuild the cached review list, followed by a cache update.
As review activity increased for popular cuisines, I thought that this design could become write-heavy and inefficient.

To address these issues, the caching design was changed to use the cache-aside pattern. Instead of updating the cache on every write, 
the cache is now updated whenever there is a read request.

\begin{verbatim}
public ReviewResponse createReview(CreateReviewRequest request) {

    Cuisine cuisine = cuisineRepository.findById(request.getCuisineId())
            .orElseThrow(() -> new IllegalArgumentException("Cuisine not found"));

    Review review = new Review(
            cuisine,
            request.getRating(),
            request.getComment()
    );

    Review saved = reviewRepository.save(review);

    String cacheKey = "cuisine:" + cuisine.getId() + ":reviews";
    cacheService.evict(cacheKey);

    return new ReviewResponse(
            saved.getId(),
            saved.getRating(),
            saved.getComment(),
            saved.getCreatedAt()
    );
}
\end{verbatim}

The difference from the write through database caching method is that new reviews are now only written in the database, and the existing cache is not evicted.
In this scope, evicted means that the original cache is deleted.
With this approach, write operations remain lightweight, consisting only of the database transaction and cache eviction.
The cache is repopulated only when a read request occurs:

\begin{verbatim}
public List<ReviewResponse> getReviewsByCuisine(Long cuisineId) {

    String cacheKey = "cuisine:" + cuisineId + ":reviews";

    Object cached = cacheService.get(cacheKey);
    if (cached != null) {
        return (List<ReviewResponse>) cached;
    }

    List<ReviewResponse> responses =
            reviewRepository.findByCuisineId(cuisineId)
                    .stream()
                    .map(r -> new ReviewResponse(
                            r.getId(),
                            r.getRating(),
                            r.getComment(),
                            r.getCreatedAt()
                    ))
                    .collect(Collectors.toList());

    cacheService.put(cacheKey, responses, 300);
    return responses;
}
\end{verbatim}

This cache-aside design reduces write overheads, improves scalability, and aligns more effectively with the read-heavy access patterns of the application.
By combining explicit cache eviction on writes with time-based expiration,
the system maintains acceptable data freshness while significantly improving performance.

Event-based cache invalidation was also considered during the design of the review caching strategy. 
In an event-based approach, cache entries are refreshed or invalidated in response to domain events published by the system.
In the case of the local cuisine application the only meaningful domain event related to reviews occurs when a new review is created for a cuisine.

An example of an event-based cache invalidation approach would involve publishing a
ReviewCreatedEvent whenever a review is persisted, and having a separate
event listener respond by evicting or updating the cached reviews for the affected cuisine.

For example, a review creation event could be created as following:

\begin{verbatim}
public class ReviewCreatedEvent {
    private final Long cuisineId;

    public ReviewCreatedEvent(Long cuisineId) {
        this.cuisineId = cuisineId;
    }

    public Long getCuisineId() {
        return cuisineId;
    }
}
\end{verbatim}

And a corresponding event listener could invalidate the cache entry:

\begin{verbatim}
@Component
public class ReviewEventListener {

    @CacheEvict(value = "reviews", key = "#event.cuisineId")
    @EventListener
    public void handleReviewCreated(ReviewCreatedEvent event) {
        // Cache is evicted in response to the event
    }
}
\end{verbatim}

However, in this system the cache eviction already occurs directly within the review creation workflow.
Because the system has a single, well-defined write path for reviews, introducing a full event-based validation mechanism would add unnecessary complexity to the application. 
The chosen approach achieves the same consistency guarantees while remaining simpler and easier to maintain.

\subsection{Backend Request Workflow}

Now that it is clear which decisions were made when building the backend of the project, this section introduces the backend workflow 
that is executed when a user makes a request to the system. It outlines the sequence of interactions between the client and the backend components, 
including the controller, service, repository, caching, and database layers. 

The following sequence diagram shows the lifecycle of a backend request starting from a user. 
It shows the order in which the client interacts with the backend components and how the request processes through the 
controller, service, caching, repository, and database layers.
This diagram focuses on a single request of a cuisine search, and highlights the responsibilities of each component involved.
While the example shown uses the cuisine module, the same request flow and architectural pattern are consistently applied across other modules in the system,
such as regions and reviews.

\seqFigure

When a user initiates a cuisine search, the backend processes the request in the following order:

\begin{enumerate}
    \item The client sends an HTTP request to the backend API.
    \item The request is received by the corresponding controller.
    \item The controller delegates the request to the service layer.
    \item The service layer checks the cache for existing data related to the request.
    \item If the requested data is found in the cache, it is returned immediately.
    \item If the data is not found or the cache entry is invalid, the service queries the repository layer.
    \item The repository retrieves the required data from the database.
    \item The service processes the retrieved entities and maps them.
    \item The processed result is stored in the cache for future requests.
    \item The controller returns the final response to the client.
\end{enumerate}


As shown in the sequential diagram above, the workflow begins when the user initiates a search action from the frontend interface.
In this example, the user searches for ``Hakata ramen''.

The frontend translates this action into an HTTP request and sends it to the backend API.

\begin{verbatim}
GET /api/cuisines/Hakata ramen
\end{verbatim}

The frontend is responsible only for user interaction and request initiation, while all business logic is handled by the backend.
The request is received by the CuisineController, which serves as the entry point to the backend.
The controller extracts request parameters and sends the request to the service layer.

\begin{verbatim}
@RestController
@RequestMapping("/api/cuisines")
public class CuisineController {

    private final CuisineService cuisineService;

    public CuisineController(CuisineService cuisineService) {
        this.cuisineService = cuisineService;
    }

    @GetMapping("/byRegion/{regionName}")
    public List<String> getByRegion(@PathVariable String regionName) {
        return cuisineService.getCuisinesByRegion(regionName);
    }
    @GetMapping("/{cuisineName}")
    public ResponseEntity<CuisineResponse> getByCuisineName(@PathVariable String cuisineName) {
        return ResponseEntity.ok(cuisineService.getCuisineByName(cuisineName));
    }
}
\end{verbatim}

The controller contains no business logic, ensuring a clear separation of the layers.

The CuisineService contains the business logic for retrieving a cuisine by name. 
It first checks the cache using the cache-aside pattern.

\begin{verbatim}
public CuisineResponse getCuisineByName(String cuisineName) {

    String cacheKey = "cuisine:name:" + cuisineName.toLowerCase();

    // 1. Check cache
    Object cached = cacheService.get(cacheKey);
    if (cached != null) {
        CuisineResponse response =
            objectMapper.convertValue(cached, CuisineResponse.class);
        System.out.println("cache hit"); 
        return response;
    }

    System.out.println("cache miss");
}
\end{verbatim}

If the cache contains a valid entry, it is converted into a CuisineResponse using ObjectMapper, which is a uitility class of a Java library used for mapping Java objects
and JSON-compatible structures.
In this contex, it transforms the cached generic object retrieved from Redis back to the CuisineRepsonse type. This will be explained more in detailed in the next step.
This avoids unnecessary database access and reduces response time.

If the cache does not contain the requested cuisine, the service queries the database through the repository:

\begin{verbatim}
Cuisine cuisine = cuisineRepository
        .findByCuisineNameIgnoreCase(cuisineName)
        .orElseThrow(() -> new RuntimeException("Cuisine not found"));

CuisineResponse response = CuisineResponse.from(cuisine);
\end{verbatim}

Here, the repository retrieves the Cuisine entity by name. If there are no matching entity, an exception is thrown and and propagated back to the controller.
The controller handles the exception and returns an appropriate HTTP error response to the frontend, typically a 404 Not Found status with a descriptive error message.
The error message is sent to the frontend, and the user is informed them that the requested cuisine could not be found.
If the cuisine data is found in the database that matches the cuisine name, the response is converted into a CuisineResponse object, which is an object that is 
responsible for transferring data to the frontend. 

\begin{verbatim}
cacheService.put(cacheKey, response, 3600);

return response;
\end{verbatim}

The response is cached with a time invalidation of 3600 seconds (1 hour) in Redis.
Finally, the controller returns the CuisineResponse to the frontend, which displays it to the user.
Below is the full FindCuisineByName method. 

\begin{verbatim}
public CuisineResponse getCuisineByName(String cuisineName) {

    String cacheKey = "cuisine:name:" + cuisineName.toLowerCase();

    // 1. Check cache
    Object cached = cacheService.get(cacheKey);
    if (cached != null) {
        CuisineResponse response =
            objectMapper.convertValue(cached, CuisineResponse.class);
    System.out.println("cache hit"); 
        return response;
    }

    System.out.println("cache miss");

    // 2. Cache miss → original logic
    Cuisine cuisine = cuisineRepository
            .findByCuisineNameIgnoreCase(cuisineName)
            .orElseThrow(() -> new RuntimeException("Cuisine not found"));

    CuisineResponse response = CuisineResponse.from(cuisine);

    // 3. Store result in cache
    cacheService.put(cacheKey, response, 3600);

    return response;
}

\end{verbatim}

\subsection{Modular Architecture Overview}

In the previous subsection, we explored the workflow for a backend request. This section will introduce indepth
what happens  how the Java classes interact with each other
to enable those workflows as a part of the modular monolithic architecture. 

The following UML class diagram provides a visual representation of the structural organization of the local cuisine application.
It illustrates how classes are organized within the system and how they interact with one another.
The diagram highlights the relationships between different classes and modules, and clarify the design and the flow of responsibilities across the application.

Different types of arrows in the diagram represent specific relationships between classes.
The six primary UML relationships and their corresponding arrow notations are as follows:

\begin{itemize}
    \item \textbf{Inheritance (Generalization):}  Solid line with a hollow triangle pointing to the parent class.
    \item \textbf{Realization (Implementation):}  Dashed line with a hollow triangle pointing to the interface.
    \item \textbf{Composition:}  Solid line with a filled diamond at the owning class. Represents a "has-a" relationship with strong ownership, where the contained object’s lifecycle depends on the owning class.
    \item \textbf{Aggregation:}  Solid line with a hollow diamond at the owning class. Indicates a weaker "has-a" relationship.
    \item \textbf{Association:}  Solid line connecting two classes, representing a structural relationship.
    \item \textbf{Dependency:}  Dashed arrow pointing from the dependent class to the class it relies on, indicating a temporary or unidirectional usage.
\end{itemize}

In this explanation, the meaning of these relationships will be clarified further in context as each module and its layered structure is discussed.

\classFigure


\textbf{Overall System Structure}


The local cuisine application is organized into three main modules, and each of them are responsible for a distinct aspect of the system.
Although these modules are logically separated, they are implemented within a single deployable Spring Boot application, forming a modular monolith. 

Each module follows a layered architecture, which promotes separation of concerns and maintainability.
This design allows the modules to evolve independently while sharing common resources,
such as the database and system configuration. The layered structure ensures a clear flow of responsibilities,
making the system easier to understand and maintain.

The layered architecture within each module organizes classes into four primary layers: Controller, Service, Repository, and Entity.
These layers clearly separate responsibilities and facilitate maintainability. The UML class diagram illustrates these layers and the relationships between the classes,
with arrows indicating dependencies and structural associations.


\textbf{Controller Layer}

The controller layer handles HTTP requests and maps endpoints to the appropriate service methods
Each controller is composed of service objects, which it depends on to execute business logic.
In UML, this relationship is represented by a composition arrow (\texttt{*--}) pointing from the controller to the service class.

For example, in the Review module, the ReviewController has a composition relationship with ReviewService.
This is implemented in Java via constructor injection: the controller receives a reference to a ReviewService instance when it is created.
This means that the controllers owns the reference to their corresponding service object for its lifetime and relies on it to handle all requests.

\begin{verbatim}
// ReviewController.java

@RestController
@RequestMapping("/api/reviews")
public class ReviewController {

    private final ReviewService reviewService;

    // Constructor injection
    public ReviewController(ReviewService reviewService) {
        this.reviewService = reviewService;
    }

    @GetMapping("/cuisine/{cuisineId}")
    public List<ReviewResponse> getReviewsByCuisine(@PathVariable Long cuisineId) {
        return reviewService.getReviewsByCuisine(cuisineId);
    }
}
\end{verbatim}

For instance, when a user makes a GET request to view reviews for a certain cuisine,
the ReviewController invokes the getReviewsByCuisine() method on its ReviewService object.
The controller does not contain any business logic itself; it passes all such responsibilities to the service.
Because the service object is initialized along with the controller and maintained as a private final field,
the UML composition arrow accurately reflects the “has-a” relationship between the controller and the service.

For the reviews module, the ReviewController interacts with data transfer objects (DTOs) for handling input and output.
DTOs are simple objects that carry data between layers of the application without containing business logic.
They are necessary to decouple the internal domain entities from external API representations,
ensuring that changes to the database model do not directly affect the API contract.

For example, a DTO used in the Review module is the CreateReviewRequest, which encapsulates the data needed to create a new review:

\begin{verbatim}
// CreateReviewRequest.java

public class CreateReviewRequest {

    @NotNull
    private Long cuisineId;

    @NotNull
    @Min(1)
    @Max(5)
    private Integer rating;

    @Size(max = 1000)
    private String comment;

    public Long getCuisineId() { return cuisineId; }
    public void setCuisineId(Long cuisineId) { this.cuisineId = cuisineId; }

    public Integer getRating() { return rating; }
    public void setRating(Integer rating) { this.rating = rating; }

    public String getComment() { return comment; }
    public void setComment(String comment) { this.comment = comment; }
}
\end{verbatim}

This DTO is used by the controller to receive data from the client in a structured format. 
It ensures that only valid and necessary data is passed to the service layer, 
allowing the internal entity objects, such as the Review entity, to remain decoupled from the API contract.
For example, the @NotNull annotation ensures that none of the fields of the submitted data isn't empty.

This class does not contain any business logic. It simply holds the data returned by the service layer. 
It is used by the ReviewController to send responses back to clients.

In the Review module, the ReviewController depends on two DTOs: CreateReviewRequest and ReviewResponse.
When a client sends a POST request to create a new review, the incoming JSON payload is mapped to a CreateReviewRequest object.
The controller then passes this object to the ReviewService for processing.
After the service completes the operation, it returns a ReviewResponse object,
which the controller sends back to the client.

\begin{verbatim}
// ReviewController.java - POST example

@PostMapping
public ReviewResponse createReview(@RequestBody @Valid CreateReviewRequest request) {
    // The controller assigns the processing to the service,
    // passing the DTO object along.
    return reviewService.createReview(request);
}
\end{verbatim}

This dependency is short-lived. The controller uses the DTO objects only during the handling of each request.
Because the controller does not own these objects for its entire lifetime, UML represents this relationship with a dependency arrow rather than a composition arrow.

\textbf{Service Layer}

The service layer contains the core business logic of the application.
Services process requests from controllers, enforce business rules, and prepare data to be returned. 
All three modules' services maintain composition relationships with their respective repository interfaces to perform database operations. 

\begin{verbatim}
// CuisineService.java - composition with repositories

@Service
public class CuisineService {

    private final CuisineRepository cuisineRepository;
    private final CacheService cacheService;
    private final ObjectMapper objectMapper;

    // Constructor injection establishes the composition relationship
    public CuisineService(CacheService cacheService, 
                          CuisineRepository cuisineRepository, 
                          ObjectMapper objectMapper) {
        this.cuisineRepository = cuisineRepository;
        this.cacheService = cacheService;
        this.objectMapper = objectMapper;
    }
}
\end{verbatim}

In this example, CuisineService maintains a composition relationship with CuisineRepository
This means that the service objects owns the reference to the corosponding repository object for its lifetime and relies on it for any database interactions.
These objects are injected via the constructor and exist for the lifetime of the service, reflecting strong ownership in UML.
Additionally, CuisineService and the ReviewService also have a composition relationship with CacheService to manage caching of frequently accessed data. 

The service layer also interacts with DTOs for input and output, similar to how controllers depend on request DTOs. 
For example, just as ReviewController depends on the CreateReviewRequest DTO to temporarily handle incoming data, 
services depend on output DTOs to return structured data to the controller. 
Here is a code example that demonstrates these relationships.

\begin{verbatim}
// CuisineService.java - dependency on DTO
public CuisineResponse getCuisineByName(String cuisineName) {

    Cuisine cuisine = cuisineRepository
            .findByCuisineNameIgnoreCase(cuisineName)
            .orElseThrow(() -> new RuntimeException("Cuisine not found"));

    // Convert entity to DTO
    return CuisineResponse.from(cuisine);
}
\end{verbatim}

\begin{verbatim}
// CuisineResponse.java - output DTO

public class CuisineResponse {

    private Long id;
    private String cuisineName;
    private String description;
    private String regionName;
    private String country;

    public CuisineResponse(Long id, String cuisineName, String description,
                           String regionName, String country) {
        this.id = id;
        this.cuisineName = cuisineName;
        this.description = description;
        this.regionName = regionName;
        this.country = country;
    }

    public static CuisineResponse from(Cuisine cuisine) {
        return new CuisineResponse(
                cuisine.getId(),
                cuisine.getCuisineName(),
                cuisine.getDescription(),
                cuisine.getRegion().getRegionName(),
                cuisine.getRegion().getCountry()
        );
    }

    public Long getId() { return id; }
    public String getCuisineName() { return cuisineName; }
    public String getDescription() { return description; }
    public String getRegionName() { return regionName; }
    public String getCountry() { return country; }
}
\end{verbatim}

Unlike composition, the service does not maintain a permanent reference to the DTO object. It only uses the DTO briefly during method execution.
In this example, the DTO is only used during when there is a get request from a client to get a specific cuisine by name from the controller.
These relationships are represented in UML by dependency arrows, indicating temporary or usage-based relationships rather than ownership. 

The CuisineResponse DTO is specifically responsible for carrying cuisine data from the service layer to the controller layer. 
It encapsulates all the fields the API should expose: id, cuisineName, description, regionName, and country. 
This allows the service to return structured data to the controller without exposing the internal Cuisine entity directly.

The CuisineResponse DTO uses the Cuisine entity to send a respond back to the Cuisine Controller.
The static from() method in CuisineResponse takes a Cuisine entity as input, reads its fields (id, cuisineName, description, and region), and constructs a new DTO object. 
Once the DTO is created and returned to the service or controller, the service no longer maintains a reference to the entity.
Because CuisineResponse only temporarily depends on the Cuisine entity during object creation, UML represents this relationship with a dependency arrow 
rather than a composition. 

\textbf{Repository Layer}

The repository layer is responsible for handling database access.
Repositories provide query methods for retrieving and storing entity objects, but they do not contain business logic. 
Their sole responsibility is to abstract data access from the service layer.
The CuisineRepository defines several query methods for retrieving 
Cuisine entities from the database:

\begin{verbatim}
// CuisineRepository.java

public interface CuisineRepository extends JpaRepository<Cuisine, Long> {
    List<Cuisine> findByRegion_RegionName(String regionName);
    List<Cuisine> findByRegion_Country(String country);
    Optional<Cuisine> findByCuisineNameIgnoreCase(String cuisineName);
}

\end{verbatim}

These repository methods are invoked within the service layer to implement 
business logic. For example, the getCuisinesByRegion() method in 
CuisineService calls findByRegion\_RegionName() to retrieve 
the corresponding entities:

\begin{verbatim}
// CuisineService.java

public List<String> getCuisinesByRegion(String regionName) {

    List<Cuisine> cuisines =
        cuisineRepository.findByRegion_RegionName(regionName);

     List<String> cuisineNames = cuisines.stream()
         .map(Cuisine::getCuisineName)
         .collect(Collectors.toList());
    
    return cuisineNames;

}
\end{verbatim}

We can see that the findByRegion\_RegionName method in the CuisineRepository 
is invoked within the CuisineService as part of the composition relationship between the two classes.
This demonstrates how the service depends on the repository to access persistent data, while still encapsulating the business logic and data 
transformation.

Similarly, ReviewService also has a composition relationship with CuisineRepository. 
This is because creating or retrieving reviews requires access to the associated Cuisine entity. 
For instance, when a new review is created, the service must first retrieve the corresponding cuisine from the database before associating it with the review. 
Therefore, both services depend on the same repository, but the repository itself remains independent of them.

\textbf{Entities}

The entity layer defines the application’s persistent data model.
Each entity maps directly to a database table and specifies the fields, relationships, and constraints that determine how data is stored and structured.

In the UML diagram, the entities are connected to reflect their relationships within the domain model.
The Region class has an aggregation relationship with the Cuisine class, indicating that a region may contain multiple cuisines. 
However, the cuisines can conceptually exist independently of the region’s lifecycle.
The multiplicity 1 near Region and 0..* near Cuisine in the UML diagram shows that each cuisine must belong to exactly one region,
while a region may have zero or more cuisines. Since a cuisine can exist without a region, there this is aggregation relationship.

In the code, this is implemented using the \texttt{@OneToMany} annotation in the Region entity:

\begin{verbatim}
// Region.java
@OneToMany(mappedBy = "region", cascade = CascadeType.ALL, fetch = FetchType.LAZY)
private List<Cuisine> cuisines;
\end{verbatim}

This code reflects the UML diagram’s relationships, showing how the Region entity contains multiple Cuisine 
entities while each Cuisine references its owning Region.
 
The Cuisine entity represents a specific type of food and maps to the cuisines table.
Each cuisine has an id, cuisineName, description, and a reference to its associated region:

\begin{verbatim}
// Cuisine.java
@ManyToOne(fetch = FetchType.LAZY)
@JoinColumn(name = "region_id", nullable = false)
private Region region;
\end{verbatim}

This establishes the many-to-one side of the region-cuisine relationship, allowing the cuisine entity to reference the region entity.
In the UML diagram, the Cuisine entity has a composition relationship with the Review entity.
Although the Cuisine entity does not explicitly maintain a collection of review objects, the relationship reflects a strong lifecycle dependency.
Each review must be associated with exactly one cuisine, and a review cannot exist independently.
The multiplicity 1 near Cuisine and 0..* near Review indicates that a cuisine may have zero or more reviews, while each review belongs to exactly one cuisine.
This representation captures the ownership and domain-level dependency between cuisines and reviews.


The Review entity saves user feedback on a cuisine and maps to the reviews table.
Each review has an id, a rating, a comment, and a timestamp. Each review is linked to a specific cuisine:

\begin{verbatim}
// Review.java
@ManyToOne(fetch = FetchType.LAZY)
@JoinColumn(name = "cuisine_id", nullable = false)
private Cuisine cuisine;
\end{verbatim}

Together, these entities form a hierarchical structure in which a Region can contain multiple Cuisine objects,
representing a one-to-many aggregation. Each Cuisine belongs to a single Region and
form a many-to-one composition relationship. Cuisines can also have multiple Review objects, indicating another one-to-many composition.
Each Review, belongs to a single Cuisine, and has the many-to-one composition relationship.
These relationships maintain integrity in the database and are accurately reflected in the UML diagram through the use of composition and aggregation arrows.

Overall, the UML class diagram provides a clear visualization of the entire backend architecture.
It shows how controllers, services, repositories, DTOs, and entities interact with each other.
It also highlights the ownership and dependency relationships between layers,
such as the composition between services and repositories, the temporary dependencies on DTOs, and the aggregation and composition among entities. 
By examining the diagram with the code examples, it becomes easier to understand the flow of data through the application,
the separation of concerns across layers, and how each component contributes to the overall functionality of the local cuisine system.
